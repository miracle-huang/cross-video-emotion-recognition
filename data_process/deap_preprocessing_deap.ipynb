{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 巴特沃斯滤波器\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data) \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件\n",
    "def read_file(file):\n",
    "    data = sio.loadmat(file)\n",
    "    data = data['data']\n",
    "    data_1st = loadmat(file)['data'][0]\n",
    "\n",
    "    # print(\"读取data_preprocessed_matlab数据,打印第一个数据\")\n",
    "    # print(data.shape)\n",
    "    # print(\"First data point:\", data_1st)\n",
    "    # print(\"\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_DE(signal):\n",
    "    # 使用 NumPy 中的 var 函数计算信号 signal 的方差。ddof=1 表示使用样本方差的无偏估计\n",
    "    variance = np.var(signal, ddof=1)\n",
    "\n",
    "    # 计算微分熵的估计值，使用方差来估计信号分布的\"广度\"\n",
    "    return math.log(2 * math.pi * math.e * variance) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose(file):\n",
    "    print(\"decompose:\") # 函数开始运行\n",
    "    # trial*channel*sample\n",
    "    # start_index 用于确定从信号中提取的时间范围的起始点。具体来说，它是用于指定从每个信号中提取的信号部分的开始位置。\n",
    "    # 这里 start_index 的值为 384，表示在每个试验信号中，从第 384 个样本点开始提取信号。\n",
    "    # 这3秒是预实验基线\n",
    "    start_index = 384  # 3s pre-trial signals 128(hz)x3(s)=384 \n",
    "    data = read_file(file)\n",
    "    # 确定信号的形状 shape 和采样频率 frequency\n",
    "    shape = data.shape\n",
    "    frequency = 128\n",
    "\n",
    "    # 用于存储处理后的微分熵数据\n",
    "    # 第一个维度（0 维）：表示数组中包含的元素数量。初始时是空数组，因此为 0。随着数据的逐步添加，这个维度会动态增长。\n",
    "    # 第二个维度（1 维）：表示每个元素的维度，即一个元素包含的子数组的个数。在这里是 4，表示每个试验信号包含四个频带的微分熵。\n",
    "    # 第三个维度（2 维）：表示每个子数组的长度，即每个频带的微分熵数据点数。在这里是 120。\n",
    "    # 这种初始化方式允许在循环中逐步添加处理后的微分熵数据，构建一个多维数组，其中每个元素代表一个试验信号的微分熵数据。\n",
    "    # 在代码的执行过程中，decomposed_de 会根据每个试验信号的微分熵数据的维度逐步增长。\n",
    "    decomposed_de = np.empty([0, 4, 120])\n",
    "    \n",
    "    # 用于存储基线信号在不同频带的微分熵数据。\n",
    "    # 第一个维度（0 维）：表示数组中包含的元素数量。初始时是空数组，因此为 0。随着数据的逐步添加，这个维度会动态增长。\n",
    "    # 第二个维度（1 维）：表示每个元素的维度，即一个元素包含的数据点数。在这里是 128，表示每个试验的基线信号在不同频带的微分熵数据点数。\n",
    "    base_DE = np.empty([0, 128])\n",
    "\n",
    "    # 在一个试验信号循环中（共有40个video）对每个通道进行处理，提取频带特征并计算微分熵。\n",
    "    for trial in range(40):\n",
    "        temp_base_DE = np.empty([0])\n",
    "        temp_base_theta_DE = np.empty([0])\n",
    "        temp_base_alpha_DE = np.empty([0])\n",
    "        temp_base_beta_DE = np.empty([0])\n",
    "        temp_base_gamma_DE = np.empty([0])\n",
    "\n",
    "        temp_de = np.empty([0, 120])\n",
    "\n",
    "        # 对于每个试验信号的每个通道，获取试验信号的部分（从第 384 个样本点开始，总共 60x128=7680 个）和基线信号的部分（前 384 个样本点）\n",
    "        # 这里的range(32) 是因为前32个channel是EEG数据\n",
    "        for channel in range(32):\n",
    "            trial_signal = data[trial, channel, start_index:]\n",
    "            base_signal = data[trial, channel, :start_index]\n",
    "            # ****************compute base DE****************\n",
    "            # 使用了巴特沃斯滤波器对基线信号进行频带滤波。\n",
    "            # 具体来说，对基线信号 base_signal 分别进行了四个频带的滤波，\n",
    "            # 分别是 theta (4-8 Hz)、alpha (8-14 Hz)、beta (14-31 Hz) 和 gamma (31-45 Hz)。\n",
    "            base_theta = butter_bandpass_filter(base_signal, 4, 8, frequency, order=3)\n",
    "            base_alpha = butter_bandpass_filter(base_signal, 8, 14, frequency, order=3)\n",
    "            base_beta = butter_bandpass_filter(base_signal, 14, 31, frequency, order=3)\n",
    "            base_gamma = butter_bandpass_filter(base_signal, 31, 45, frequency, order=3)\n",
    "\n",
    "            # 计算了基线信号在不同频带下的平均微分熵 (base_theta_DE, base_alpha_DE, base_beta_DE, base_gamma_DE)。\n",
    "            # 它首先将每个频带划分为六个子段，每段0.5s，然后计算每个子段的微分熵，最后取平均值\n",
    "            base_theta_DE = (compute_DE(base_theta[:64]) + compute_DE(base_theta[64:128]) + compute_DE(\n",
    "                base_theta[128:192]) + compute_DE(base_theta[192:256]) + compute_DE(base_theta[256:320]) + compute_DE(\n",
    "                base_theta[320:])) / 6\n",
    "            base_alpha_DE = (compute_DE(base_alpha[:64]) + compute_DE(base_alpha[64:128]) + compute_DE(\n",
    "                base_alpha[128:192]) + compute_DE(base_theta[192:256]) + compute_DE(base_theta[256:320]) + compute_DE(\n",
    "                base_theta[320:])) / 6\n",
    "            base_beta_DE = (compute_DE(base_beta[:64]) + compute_DE(base_beta[64:128]) + compute_DE(\n",
    "                base_beta[128:192]) + compute_DE(base_theta[192:256]) + compute_DE(base_theta[256:320]) + compute_DE(\n",
    "                base_theta[320:])) / 6\n",
    "            base_gamma_DE = (compute_DE(base_gamma[:64]) + compute_DE(base_gamma[64:128]) + compute_DE(\n",
    "                base_gamma[128:192]) + compute_DE(base_theta[192:256]) + compute_DE(base_theta[256:320]) + compute_DE(\n",
    "                base_theta[320:])) / 6\n",
    "\n",
    "            # 将基线信号在不同频带的微分熵数据添加到相应的临时数组中\n",
    "            temp_base_theta_DE = np.append(temp_base_theta_DE, base_theta_DE)\n",
    "            temp_base_gamma_DE = np.append(temp_base_gamma_DE, base_gamma_DE)\n",
    "            temp_base_beta_DE = np.append(temp_base_beta_DE, base_beta_DE)\n",
    "            temp_base_alpha_DE = np.append(temp_base_alpha_DE, base_alpha_DE)\n",
    "\n",
    "            # 对试验信号进行巴特沃斯带通滤波，原理和基线信号一样\n",
    "            theta = butter_bandpass_filter(trial_signal, 4, 8, frequency, order=3)\n",
    "            alpha = butter_bandpass_filter(trial_signal, 8, 14, frequency, order=3)\n",
    "            beta = butter_bandpass_filter(trial_signal, 14, 31, frequency, order=3)\n",
    "            gamma = butter_bandpass_filter(trial_signal, 31, 45, frequency, order=3)\n",
    "\n",
    "            # 将这四个数组初始化为全零数组，用于存储试验信号在不同频带下的微分熵数据\n",
    "            DE_theta = np.zeros(shape=[0], dtype=float)\n",
    "            DE_alpha = np.zeros(shape=[0], dtype=float)\n",
    "            DE_beta = np.zeros(shape=[0], dtype=float)\n",
    "            DE_gamma = np.zeros(shape=[0], dtype=float)\n",
    "\n",
    "            # 这里实际上是把试验信号分成120个相对短的时间段或窗口，然后对每个窗口进行微分熵的计算。\n",
    "            # 每个窗口的长度为64个样本点，窗口持续时间为0.5秒\n",
    "            for index in range(120):\n",
    "                DE_theta = np.append(DE_theta, compute_DE(theta[index * 64:(index + 1) * 64]))\n",
    "                DE_alpha = np.append(DE_alpha, compute_DE(alpha[index * 64:(index + 1) * 64]))\n",
    "                DE_beta = np.append(DE_beta, compute_DE(beta[index * 64:(index + 1) * 64]))\n",
    "                DE_gamma = np.append(DE_gamma, compute_DE(gamma[index * 64:(index + 1) * 64]))\n",
    "\n",
    "            # 通过 np.vstack 函数将每个频带下的微分熵数据（DE_theta、DE_alpha、DE_beta、DE_gamma）\n",
    "            # 垂直堆叠在一起，形成一个临时的二维数组 temp_de。这个数组的每一行代表一个频带下的微分熵数据。\n",
    "            temp_de = np.vstack([temp_de, DE_theta])\n",
    "            temp_de = np.vstack([temp_de, DE_alpha])\n",
    "            temp_de = np.vstack([temp_de, DE_beta])\n",
    "            temp_de = np.vstack([temp_de, DE_gamma])\n",
    "\n",
    "            # print(\"temp_de shape:\", temp_de.shape)\n",
    "\n",
    "        temp_trial_de = temp_de.reshape(-1, 4, 120)\n",
    "        # print(\"temp_trial_de shape:\", temp_trial_de.shape)\n",
    "        decomposed_de = np.vstack([decomposed_de, temp_trial_de])\n",
    "        # print(\"decomposed_de shape:\", decomposed_de.shape)\n",
    "\n",
    "        temp_base_DE = np.append(temp_base_theta_DE, temp_base_alpha_DE)\n",
    "        temp_base_DE = np.append(temp_base_DE, temp_base_beta_DE)\n",
    "        temp_base_DE = np.append(temp_base_DE, temp_base_gamma_DE)\n",
    "        # base_DE 将包含所有通道在四个频带下的基线微分熵数据。\n",
    "        base_DE = np.vstack([base_DE, temp_base_DE])\n",
    "        # print(\"temp_base_DE:\", temp_base_DE.shape)\n",
    "        print(\"base_DE:\", base_DE.shape)\n",
    "\n",
    "    # print(\"decomposed_de before reshape:\", decomposed_de.shape)\n",
    "    #40 视频 x 60秒/视频 / 0.5s (窗口) = 4800\n",
    "    # decomposed_de = decomposed_de.reshape(-1, 32, 4, 120).transpose([0, 3, 2, 1]).reshape(-1, 4, 32).reshape(-1, 128)\n",
    "    # 在这里要把每个视频的数据分开来计算\n",
    "    decomposed_de = decomposed_de.reshape(-1, 32, 4, 120).transpose([0, 3, 2, 1]).reshape(40, 120, -1) # (40, 120, 128)\n",
    "    \n",
    "    print(\"base_DE shape:\", base_DE.shape)\n",
    "    print(\"trial_DE shape:\", decomposed_de.shape)\n",
    "    print(\"\")\n",
    "    return base_DE, decomposed_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PSD(signal):\n",
    "    # do PSD not DE\n",
    "    if True:\n",
    "        return np.sum(signal**2)\n",
    "    else:\n",
    "        variance = np.var(signal, ddof=1)\n",
    "        return math.log(2 * math.pi * math.e * variance) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_PSD(file):\n",
    "    print(\"decompose:\") # 函数开始运行\n",
    "    # 这3秒是预实验基线\n",
    "    start_index = 384  # 3s pre-trial signals 128(hz)x3(s)=384 \n",
    "    data = read_file(file)\n",
    "    # 确定信号的形状 shape 和采样频率 frequency\n",
    "    shape = data.shape\n",
    "    frequency = 128\n",
    "\n",
    "    # 用于存储处理后的功率谱密度数据\n",
    "    decomposed_de = np.empty([0, 4, 120])\n",
    "    \n",
    "    # 用于存储基线信号在不同频带的功率谱密度数据。\n",
    "    base_DE = np.empty([0, 128])\n",
    "\n",
    "    # 在一个试验信号循环中（共有40个video）对每个通道进行处理，提取频带特征并计算功率谱密度。\n",
    "    for trial in range(40):\n",
    "        temp_base_DE = np.empty([0])\n",
    "        temp_base_theta_DE = np.empty([0])\n",
    "        temp_base_alpha_DE = np.empty([0])\n",
    "        temp_base_beta_DE = np.empty([0])\n",
    "        temp_base_gamma_DE = np.empty([0])\n",
    "\n",
    "        temp_de = np.empty([0, 120])\n",
    "\n",
    "        # 对于每个试验信号的每个通道，获取试验信号的部分（从第 384 个样本点开始，总共 60x128=7680 个）和基线信号的部分（前 384 个样本点）\n",
    "        # 这里的range(32) 是因为前32个channel是EEG数据\n",
    "        for channel in range(32):\n",
    "            trial_signal = data[trial, channel, start_index:]\n",
    "            base_signal = data[trial, channel, :start_index]\n",
    "            # ****************compute base DE****************\n",
    "            # 使用了巴特沃斯滤波器对基线信号进行频带滤波。\n",
    "            # 具体来说，对基线信号 base_signal 分别进行了四个频带的滤波，\n",
    "            # 分别是 theta (4-8 Hz)、alpha (8-14 Hz)、beta (14-31 Hz) 和 gamma (31-45 Hz)。\n",
    "            base_theta = butter_bandpass_filter(base_signal, 4, 8, frequency, order=3)\n",
    "            base_alpha = butter_bandpass_filter(base_signal, 8, 14, frequency, order=3)\n",
    "            base_beta = butter_bandpass_filter(base_signal, 14, 31, frequency, order=3)\n",
    "            base_gamma = butter_bandpass_filter(base_signal, 31, 45, frequency, order=3)\n",
    "\n",
    "            # 计算了基线信号在不同频带下的平均微分熵 (base_theta_DE, base_alpha_DE, base_beta_DE, base_gamma_DE)。\n",
    "            # 它首先将每个频带划分为六个子段，每段0.5s，然后计算每个子段的微分熵，最后取平均值\n",
    "            base_theta_DE = (compute_PSD(base_theta[:64]) + compute_PSD(base_theta[64:128]) + compute_PSD(\n",
    "                base_theta[128:192]) + compute_PSD(base_theta[192:256]) + compute_PSD(base_theta[256:320]) + compute_PSD(\n",
    "                base_theta[320:])) / 6\n",
    "            base_alpha_DE = (compute_PSD(base_alpha[:64]) + compute_PSD(base_alpha[64:128]) + compute_PSD(\n",
    "                base_alpha[128:192]) + compute_PSD(base_theta[192:256]) + compute_PSD(base_theta[256:320]) + compute_PSD(\n",
    "                base_theta[320:])) / 6\n",
    "            base_beta_DE = (compute_PSD(base_beta[:64]) + compute_PSD(base_beta[64:128]) + compute_PSD(\n",
    "                base_beta[128:192]) + compute_PSD(base_theta[192:256]) + compute_PSD(base_theta[256:320]) + compute_PSD(\n",
    "                base_theta[320:])) / 6\n",
    "            base_gamma_DE = (compute_PSD(base_gamma[:64]) + compute_PSD(base_gamma[64:128]) + compute_PSD(\n",
    "                base_gamma[128:192]) + compute_PSD(base_theta[192:256]) + compute_PSD(base_theta[256:320]) + compute_PSD(\n",
    "                base_theta[320:])) / 6\n",
    "\n",
    "            # 将基线信号在不同频带的微分熵数据添加到相应的临时数组中\n",
    "            temp_base_theta_DE = np.append(temp_base_theta_DE, base_theta_DE)\n",
    "            temp_base_gamma_DE = np.append(temp_base_gamma_DE, base_gamma_DE)\n",
    "            temp_base_beta_DE = np.append(temp_base_beta_DE, base_beta_DE)\n",
    "            temp_base_alpha_DE = np.append(temp_base_alpha_DE, base_alpha_DE)\n",
    "\n",
    "            # 对试验信号进行巴特沃斯带通滤波，原理和基线信号一样\n",
    "            theta = butter_bandpass_filter(trial_signal, 4, 8, frequency, order=3)\n",
    "            alpha = butter_bandpass_filter(trial_signal, 8, 14, frequency, order=3)\n",
    "            beta = butter_bandpass_filter(trial_signal, 14, 31, frequency, order=3)\n",
    "            gamma = butter_bandpass_filter(trial_signal, 31, 45, frequency, order=3)\n",
    "\n",
    "            # 将这四个数组初始化为全零数组，用于存储试验信号在不同频带下的微分熵数据\n",
    "            DE_theta = np.zeros(shape=[0], dtype=float)\n",
    "            DE_alpha = np.zeros(shape=[0], dtype=float)\n",
    "            DE_beta = np.zeros(shape=[0], dtype=float)\n",
    "            DE_gamma = np.zeros(shape=[0], dtype=float)\n",
    "\n",
    "            # 这里实际上是把试验信号分成120个相对短的时间段或窗口，然后对每个窗口进行微分熵的计算。\n",
    "            # 每个窗口的长度为64个样本点，窗口持续时间为0.5秒\n",
    "            for index in range(120):\n",
    "                DE_theta = np.append(DE_theta, compute_PSD(theta[index * 64:(index + 1) * 64]))\n",
    "                DE_alpha = np.append(DE_alpha, compute_PSD(alpha[index * 64:(index + 1) * 64]))\n",
    "                DE_beta = np.append(DE_beta, compute_PSD(beta[index * 64:(index + 1) * 64]))\n",
    "                DE_gamma = np.append(DE_gamma, compute_PSD(gamma[index * 64:(index + 1) * 64]))\n",
    "\n",
    "            # 通过 np.vstack 函数将每个频带下的微分熵数据（DE_theta、DE_alpha、DE_beta、DE_gamma）\n",
    "            # 垂直堆叠在一起，形成一个临时的二维数组 temp_de。这个数组的每一行代表一个频带下的微分熵数据。\n",
    "            temp_de = np.vstack([temp_de, DE_theta])\n",
    "            temp_de = np.vstack([temp_de, DE_alpha])\n",
    "            temp_de = np.vstack([temp_de, DE_beta])\n",
    "            temp_de = np.vstack([temp_de, DE_gamma])\n",
    "\n",
    "            # print(\"temp_de shape:\", temp_de.shape)\n",
    "\n",
    "        temp_trial_de = temp_de.reshape(-1, 4, 120)\n",
    "        # print(\"temp_trial_de shape:\", temp_trial_de.shape)\n",
    "        decomposed_de = np.vstack([decomposed_de, temp_trial_de])\n",
    "        # print(\"decomposed_de shape:\", decomposed_de.shape)\n",
    "\n",
    "        temp_base_DE = np.append(temp_base_theta_DE, temp_base_alpha_DE)\n",
    "        temp_base_DE = np.append(temp_base_DE, temp_base_beta_DE)\n",
    "        temp_base_DE = np.append(temp_base_DE, temp_base_gamma_DE)\n",
    "        # base_DE 将包含所有通道在四个频带下的基线微分熵数据。\n",
    "        base_DE = np.vstack([base_DE, temp_base_DE])\n",
    "        # print(\"temp_base_DE:\", temp_base_DE.shape)\n",
    "        # print(\"base_DE:\", base_DE.shape)\n",
    "\n",
    "    # print(\"decomposed_de before reshape:\", decomposed_de.shape)\n",
    "    #40 视频 x 60秒/视频 / 0.5s (窗口) = 4800\n",
    "    # decomposed_de = decomposed_de.reshape(-1, 32, 4, 120).transpose([0, 3, 2, 1]).reshape(-1, 4, 32).reshape(-1, 128)\n",
    "    # 在这里要把每个视频的数据分开来计算\n",
    "    decomposed_de = decomposed_de.reshape(-1, 32, 4, 120).transpose([0, 3, 2, 1]).reshape(40, 120, -1) # (40, 120, 128)\n",
    "    \n",
    "    print(\"base_PSD shape:\", base_DE.shape)\n",
    "    print(\"trial_PSD shape:\", decomposed_de.shape)\n",
    "    print(\"\")\n",
    "    return base_DE, decomposed_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(file):\n",
    "    # 0 valence, 1 arousal, 2 dominance, 3 liking\n",
    "    valence_labels = sio.loadmat(file)[\"labels\"][:, 0] > 5  # valence labels\n",
    "    arousal_labels = sio.loadmat(file)[\"labels\"][:, 1] > 5  # arousal labels\n",
    "    valence_labels = valence_labels.astype(int)\n",
    "    arousal_labels = arousal_labels.astype(int)\n",
    "    \n",
    "    final_valence_labels = np.empty((40, 120))\n",
    "    final_arousal_labels = np.empty((40, 120))\n",
    "    \n",
    "    for i in range(0, 40):\n",
    "        final_valence_labels[i, :] = valence_labels[i]\n",
    "        final_arousal_labels[i, :] = arousal_labels[i]\n",
    "    print(\"get_labels:\")\n",
    "    print(\"labels:\", final_arousal_labels.shape)\n",
    "    return final_arousal_labels, final_valence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  s01.mat ......\n",
      "decompose:\n",
      "base_DE: (1, 128)\n",
      "base_DE: (2, 128)\n",
      "base_DE: (3, 128)\n",
      "base_DE: (4, 128)\n",
      "base_DE: (5, 128)\n",
      "base_DE: (6, 128)\n",
      "base_DE: (7, 128)\n",
      "base_DE: (8, 128)\n",
      "base_DE: (9, 128)\n",
      "base_DE: (10, 128)\n",
      "base_DE: (11, 128)\n",
      "base_DE: (12, 128)\n",
      "base_DE: (13, 128)\n",
      "base_DE: (14, 128)\n",
      "base_DE: (15, 128)\n",
      "base_DE: (16, 128)\n",
      "base_DE: (17, 128)\n",
      "base_DE: (18, 128)\n",
      "base_DE: (19, 128)\n",
      "base_DE: (20, 128)\n",
      "base_DE: (21, 128)\n",
      "base_DE: (22, 128)\n",
      "base_DE: (23, 128)\n",
      "base_DE: (24, 128)\n",
      "base_DE: (25, 128)\n",
      "base_DE: (26, 128)\n",
      "base_DE: (27, 128)\n",
      "base_DE: (28, 128)\n",
      "base_DE: (29, 128)\n",
      "base_DE: (30, 128)\n",
      "base_DE: (31, 128)\n",
      "base_DE: (32, 128)\n",
      "base_DE: (33, 128)\n",
      "base_DE: (34, 128)\n",
      "base_DE: (35, 128)\n",
      "base_DE: (36, 128)\n",
      "base_DE: (37, 128)\n",
      "base_DE: (38, 128)\n",
      "base_DE: (39, 128)\n",
      "base_DE: (40, 128)\n",
      "base_DE shape: (40, 128)\n",
      "trial_DE shape: (40, 120, 128)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29612\\2708032933.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mbase_DE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial_DE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0marousal_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalence_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m# sio.savemat(result_dir + \"DE_\" + file,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#             {\"base_data\": base_DE, \"data\": trial_DE, \"valence_labels\": valence_labels,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29612\\31180834.py\u001b[0m in \u001b[0;36mget_labels\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mfinal_valence_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalence_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mfinal_arousal_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marousal_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get_labels:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"labels:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_arousal_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29612\\31180834.py\u001b[0m in \u001b[0;36mget_labels\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mfinal_valence_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalence_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mfinal_arousal_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marousal_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get_labels:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"labels:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_arousal_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2069\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2070\u001b[1;33m                 \u001b[0mkeep_suspended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframes_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         \u001b[0mframes_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2105\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2106\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2108\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 输出de文件\n",
    "dataset_dir = \"raw_data\"\n",
    "\n",
    "result_dir = \"preprocess_step1/de/\"\n",
    "if os.path.isdir(result_dir) == False:\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "for file in os.listdir(dataset_dir):\n",
    "    print(\"processing: \", file, \"......\")\n",
    "    file_path = os.path.join(dataset_dir, file)\n",
    "    base_DE, trial_DE = decompose(file_path)\n",
    "    arousal_labels, valence_labels = get_labels(file_path)\n",
    "    sio.savemat(result_dir + \"DE_\" + file,\n",
    "                {\"base_data\": base_DE, \"data\": trial_DE, \"valence_labels\": valence_labels,\n",
    "                    \"arousal_labels\": arousal_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出psd文件\n",
    "dataset_dir = \"E:/dataset/deap_dataset/data_preprocessed_matlab/\"\n",
    "\n",
    "result_dir = \"E:/dataset/deap_dataset/preprocessing_data/psd/\"\n",
    "if os.path.isdir(result_dir) == False:\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "for file in os.listdir(dataset_dir):\n",
    "    print(\"processing: \", file, \"......\")\n",
    "    file_path = os.path.join(dataset_dir, file)\n",
    "    base_PSD, trial_PSD = decompose_PSD(file_path)\n",
    "    arousal_labels, valence_labels = get_labels(file_path)\n",
    "    sio.savemat(result_dir + \"PSD_\" + file,\n",
    "                {\"base_data\": base_PSD, \"data\": trial_PSD, \"valence_labels\": valence_labels,\n",
    "                    \"arousal_labels\": arousal_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data(40*120*128)\n",
    "# base_data(40*128)\n",
    "# arousal_labels(40*120)\n",
    "# valence_labels(40*120)\n",
    "def read_file(file):\n",
    "    file = sio.loadmat(file)\n",
    "    trial_data = file['data']\n",
    "    base_data = file[\"base_data\"]\n",
    "    return trial_data, base_data, file[\"arousal_labels\"], file[\"valence_labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 计算试验数据和基准数据之间的偏移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_deviation(vector1, vector2):\n",
    "    return vector1 - vector2\n",
    "\n",
    "def get_dataset_deviation(trial_data, base_data):\n",
    "    new_dataset = np.empty([0, 120, 128])\n",
    "    for i in range(0, 40):\n",
    "        new_record = np.array([get_vector_deviation(trial_data[i][j], base_data[i]) for j in range(0, 120)])\n",
    "        # print(\"new_record shape:\", new_record.shape)\n",
    "        new_record = new_record[np.newaxis, :, :]  # 添加一个额外的维度\n",
    "        new_dataset = np.vstack([new_dataset, new_record])\n",
    "    # print(\"get_dataset_deviation:\")\n",
    "    # print(\"new_dataset shape:\", new_dataset.shape) # new_dataset shape: (40, 120, 128)\n",
    "    # print(\"new_dataset:\" ,new_dataset)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_1Dto2D(data, Y=8, X=9):\n",
    "\n",
    "    # print(\"data_1Dto2D data shape:\", data.shape)\n",
    "\n",
    "    data_2D = np.zeros([Y, X])\n",
    "    data_2D[0] = (0, 0, data[1], data[0], 0, data[16], data[17], 0, 0)\n",
    "    data_2D[1] = (data[3], 0, data[2], 0, data[18], 0, data[19], 0, data[20])\n",
    "    data_2D[2] = (0, data[4], 0, data[5], 0, data[22], 0, data[21], 0)\n",
    "    data_2D[3] = (data[7], 0, data[6], 0, data[23], 0, data[24], 0, data[25])\n",
    "    data_2D[4] = (0, data[8], 0, data[9], 0, data[27], 0, data[26], 0)\n",
    "    data_2D[5] = (data[11], 0, data[10], 0, data[15], 0, data[28], 0, data[29])\n",
    "    data_2D[6] = (0, 0, 0, data[12], 0, data[30], 0, 0, 0)\n",
    "    data_2D[7] = (0, 0, 0, data[13], data[14], data[31], 0, 0, 0)\n",
    "    # return shape:9*9\n",
    "    return data_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(path, y_n):\n",
    "    # DE feature vector dimension of each band\n",
    "    data_3D = np.empty([0, 120, 4, 8, 9])\n",
    "    sub_vector_len = 32\n",
    "    trial_data, base_data, arousal_labels, valence_labels = read_file(path)\n",
    "    if y_n == \"yes\":\n",
    "        data = get_dataset_deviation(trial_data, base_data)\n",
    "\n",
    "        # 将三维数组转换为二维数组\n",
    "        reshaped_data = data.reshape(-1, data.shape[-1])\n",
    "        # 对二维数组进行标准化，axis=1 表示按行标准化\n",
    "        scaled_data = preprocessing.scale(reshaped_data, axis=1, with_mean=True, with_std=True, copy=True)\n",
    "        # 将标准化后的二维数组重新转回三维数组\n",
    "        data = scaled_data.reshape(data.shape)\n",
    "    else:\n",
    "        reshaped_trial_data = trial_data.reshape(-1, trial_data.shape[-1])\n",
    "        scaled_trial_data = preprocessing.scale(reshaped_trial_data, axis=1, with_mean=True, with_std=True, copy=True)\n",
    "        data = scaled_trial_data.reshape(data.shape)\n",
    "    # convert 128 vector ---> 4*9*9 cube\n",
    "    # data(40*120*128)\n",
    "    for vector in data:\n",
    "        temp_data = np.empty((0, 4, 8, 9))\n",
    "        for i in range(0, 120):\n",
    "            vector_3D = np.empty((0, 8, 9))  # 初始化一个空的二维数组，用于存放当前 vector 的处理结果\n",
    "            for band in range(0, 4):\n",
    "                data_2D_temp = data_1Dto2D(vector[i][band * sub_vector_len:(band + 1) * sub_vector_len])\n",
    "                # print(\"data_2D_temp:\", data_2D_temp)\n",
    "                # print(\"data_2D_temp shape:\", data_2D_temp.shape) # data_2D_temp shape: (8, 9)\n",
    "                data_2D_temp = data_2D_temp.reshape(1, 8, 9)\n",
    "                vector_3D = np.vstack([vector_3D, data_2D_temp])\n",
    "            vector_3D = vector_3D.reshape(1, 4, 8, 9)\n",
    "            temp_data = np.vstack([temp_data, vector_3D])\n",
    "        temp_data = temp_data.reshape(1, 120, 4, 8, 9)\n",
    "        data_3D = np.vstack([data_3D, temp_data])\n",
    "    # print(\"final data shape:\", data_3D.shape)\n",
    "    # print(\"final data:\", data_3D)\n",
    "    return data_3D, arousal_labels, valence_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DE数据40个文件，PSD数据40个文件，最终的文件夹当中应当有80个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  DE_s01.mat ......\n",
      "processing:  DE_s02.mat ......\n",
      "processing:  DE_s03.mat ......\n",
      "processing:  DE_s04.mat ......\n",
      "processing:  DE_s05.mat ......\n",
      "processing:  DE_s06.mat ......\n",
      "processing:  DE_s07.mat ......\n",
      "processing:  DE_s08.mat ......\n",
      "processing:  DE_s09.mat ......\n",
      "processing:  DE_s10.mat ......\n",
      "processing:  DE_s11.mat ......\n",
      "processing:  DE_s12.mat ......\n",
      "processing:  DE_s13.mat ......\n",
      "processing:  DE_s14.mat ......\n",
      "processing:  DE_s15.mat ......\n",
      "processing:  DE_s16.mat ......\n",
      "processing:  DE_s17.mat ......\n",
      "processing:  DE_s18.mat ......\n",
      "processing:  DE_s19.mat ......\n",
      "processing:  DE_s20.mat ......\n",
      "processing:  DE_s21.mat ......\n",
      "processing:  DE_s22.mat ......\n",
      "processing:  DE_s23.mat ......\n",
      "processing:  DE_s24.mat ......\n",
      "processing:  DE_s25.mat ......\n",
      "processing:  DE_s26.mat ......\n",
      "processing:  DE_s27.mat ......\n",
      "processing:  DE_s28.mat ......\n",
      "processing:  DE_s29.mat ......\n",
      "processing:  DE_s30.mat ......\n",
      "processing:  DE_s31.mat ......\n",
      "processing:  DE_s32.mat ......\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20908\\115616475.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mfinal_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mfinal_arousal_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_arousal_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mfinal_valence_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_valence_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# final_arousal_labels = final_arousal_labels.transpose([1, 0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20908\\115616475.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mfinal_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mfinal_arousal_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_arousal_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mfinal_valence_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_valence_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# final_arousal_labels = final_arousal_labels.transpose([1, 0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2069\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2070\u001b[1;33m                 \u001b[0mkeep_suspended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframes_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         \u001b[0mframes_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2105\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2106\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2108\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 处理de\n",
    "dataset_dir = \"E:/dataset/deap_dataset/preprocessing_data/de\"\n",
    "use_baseline = \"yes\"\n",
    "if use_baseline == \"yes\":\n",
    "    result_dir = \"E:/dataset/deap_dataset/preprocessing_data/with_base_0.5/\"\n",
    "    if os.path.isdir(result_dir) == False:\n",
    "        os.makedirs(result_dir)\n",
    "else:\n",
    "    result_dir = \"E:/dataset/deap_dataset/preprocessing_data/without_base_0.5/\"\n",
    "    if os.path.isdir(result_dir) == False:\n",
    "        os.makedirs(result_dir)\n",
    "\n",
    "final_data = np.empty((0, 40, 4, 8, 9))\n",
    "final_arousal_labels = np.empty((0, 40))\n",
    "final_valence_labels = np.empty((0, 40))\n",
    "for file in os.listdir(dataset_dir):\n",
    "    print(\"processing: \", file, \"......\")\n",
    "    file_path = os.path.join(dataset_dir, file)\n",
    "    data, arousal_labels, valence_labels = pre_process(file_path, use_baseline)\n",
    "    data = data.transpose([1, 0, 2, 3, 4])\n",
    "    final_data = np.vstack([final_data, data])\n",
    "    # print(\"1 person shape:\", data.shape)\n",
    "    # print(\"final shape:\", final_data.shape)\n",
    "    arousal_labels = arousal_labels.transpose([1, 0])\n",
    "    valence_labels = valence_labels.transpose([1, 0])\n",
    "    final_arousal_labels = np.vstack([final_arousal_labels, arousal_labels])\n",
    "    final_valence_labels = np.vstack([final_valence_labels, valence_labels])\n",
    "    # print(\"arousal shape:\", arousal_labels.shape)\n",
    "    # print(\"valence shape:\", valence_labels.shape)\n",
    "    # break\n",
    "final_data = final_data.transpose([1, 0, 2, 3, 4])\n",
    "final_arousal_labels = final_arousal_labels.transpose([1, 0]) # 在这里是一个153600的一维数组,0和1分别代表label\n",
    "final_valence_labels = final_valence_labels.transpose([1, 0])\n",
    "\n",
    "# final_arousal_labels = final_arousal_labels.transpose([1, 0])\n",
    "# final_valence_labels = final_valence_labels.transpose([1, 0])\n",
    "# print(\"final shape:\", final_data.shape) # (40, 120, 4, 8, 9)\n",
    "# for video in range(0, 40):\n",
    "#         print(\"DE_video\", str(video + 1).zfill(2), \"is saving ......\")\n",
    "#         sio.savemat(result_dir + \"DE_video\" + str(video + 1).zfill(2) + \".mat\",\n",
    "#                 {\"data\": final_data[video], \"valence_labels\": final_valence_labels[video], \"arousal_labels\": final_arousal_labels[video]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理PSD\n",
    "dataset_dir = \"E:/dataset/deap_dataset/preprocessing_data/psd\"\n",
    "use_baseline = \"yes\"\n",
    "if use_baseline == \"yes\":\n",
    "    result_dir = \"E:/dataset/deap_dataset/preprocessing_data/with_base_0.5/\"\n",
    "    if os.path.isdir(result_dir) == False:\n",
    "        os.makedirs(result_dir)\n",
    "else:\n",
    "    result_dir = \"E:/dataset/deap_dataset/preprocessing_data/without_base_0.5/\"\n",
    "    if os.path.isdir(result_dir) == False:\n",
    "        os.makedirs(result_dir)\n",
    "\n",
    "final_data = np.empty((0, 40, 4, 8, 9))\n",
    "final_arousal_labels = np.empty((0, 40))\n",
    "final_valence_labels = np.empty((0, 40))\n",
    "for file in os.listdir(dataset_dir):\n",
    "    print(\"processing: \", file, \"......\")\n",
    "    file_path = os.path.join(dataset_dir, file)\n",
    "    data, arousal_labels, valence_labels = pre_process(file_path, use_baseline)\n",
    "    data = data.transpose([1, 0, 2, 3, 4])\n",
    "    final_data = np.vstack([final_data, data])\n",
    "    # print(\"1 person shape:\", data.shape)\n",
    "    # print(\"final shape:\", final_data.shape)\n",
    "    arousal_labels = arousal_labels.transpose([1, 0])\n",
    "    valence_labels = valence_labels.transpose([1, 0])\n",
    "    final_arousal_labels = np.vstack([final_arousal_labels, arousal_labels])\n",
    "    final_valence_labels = np.vstack([final_valence_labels, valence_labels])\n",
    "    # print(\"arousal shape:\", arousal_labels.shape)\n",
    "    # print(\"valence shape:\", valence_labels.shape)\n",
    "    # break\n",
    "final_data = final_data.transpose([1, 0, 2, 3, 4])\n",
    "final_arousal_labels = final_arousal_labels.transpose([1, 0])\n",
    "final_valence_labels = final_valence_labels.transpose([1, 0])\n",
    "\n",
    "# final_arousal_labels = final_arousal_labels.transpose([1, 0])\n",
    "# final_valence_labels = final_valence_labels.transpose([1, 0])\n",
    "# print(\"final shape:\", final_data.shape) # (40, 120, 4, 8, 9)\n",
    "for video in range(0, 40):\n",
    "        print(\"PSD_video\", str(video + 1).zfill(2), \"is saving ......\")\n",
    "        sio.savemat(result_dir + \"PSD_video\" + str(video + 1).zfill(2) + \".mat\",\n",
    "                {\"data\": final_data[video], \"valence_labels\": final_valence_labels[video], \"arousal_labels\": final_arousal_labels[video]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
